+ mkdir /root/data/
mkdir: cannot create directory '/root/data/': File exists
+ cp -r ./data/math /root/data/math
+ export ACCELERATE_LOG_LEVEL=info
+ ACCELERATE_LOG_LEVEL=info
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ export WANDB_CONSOLE=off
+ WANDB_CONSOLE=off
+ export WANDB_MODE=offline
+ WANDB_MODE=offline
+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ ray stop --force
2025-09-07 13:37:41,238	INFO scripts.py:1287 -- Did not find any active Ray processes.
+ export PET_NODE_RANK=0
+ PET_NODE_RANK=0
+ export WORLD_SIZE=1
+ WORLD_SIZE=1
+ PROJECT_NAME=self_rl_math
+ RUN_NAME=grpo_mv_naive
++ date +%Y-%m-%d_%H-%M-%S
+ TIMESTAMP=2025-09-07_13-37-41
+ EXPERIMENT_NAME=self_rl_math-grpo_mv_naive-2025-09-07_13-37-41
+ SAVE_CHECKPOINT_DIR=output_dir
+ EXP_DIR=output_dir/self_rl_math/self_rl_math-grpo_mv_naive-2025-09-07_13-37-41
+ today=905
+ mkdir running_logs/905
mkdir: cannot create directory 'running_logs/905': File exists
+ mkdir -p output_dir
+ mkdir -p output_dir/self_rl_math
+ mkdir -p output_dir/self_rl_math/self_rl_math-grpo_mv_naive-2025-09-07_13-37-41
+ mkdir -p output_dir/self_rl_math/self_rl_math-grpo_mv_naive-2025-09-07_13-37-41
+ mkdir -p output_dir/logs/tensorboard
+ mkdir -p output_dir/logs/rl_logging_board
+ PYTHONUNBUFFERED=1
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=/root/data/math/train.parquet data.val_files=/root/data/math/test.parquet data.train_batch_size=128 data.max_prompt_length=512 data.max_response_length=3072 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=/inspire/hdd/project/robot-dna/sujiadi-p-sujiadi/baibizhe-tmp/ckpts/llm/Qwen2.5-3B actor_rollout_ref.model.use_fused_kernels=False actor_rollout_ref.actor.optim.lr=3e-6 actor_rollout_ref.actor.optim.warmup_style=cosine actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.1 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=128 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.005 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.3 actor_rollout_ref.rollout.n=12 actor_rollout_ref.rollout.free_cache_engine=False actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.n_gpus_per_node=8 reward_model.reward_manager=m_grpo trainer.nnodes=1 'trainer.logger=[console,wandb,tensorboard]' trainer.project_name=verl trainer.save_freq=10 trainer.val_before_train=False trainer.test_freq=10 trainer.project_name=self_rl_math trainer.experiment_name=self_rl_math-grpo_mv_naive-2025-09-07_13-37-41 trainer.default_local_dir=output_dir/self_rl_math/self_rl_math-grpo_mv_naive-2025-09-07_13-37-41 +trainer.tensorboard_dir=output_dir/logs/tensorboard +trainer.rl_logging_board_dir=output_dir/logs/rl_logging_board trainer.total_epochs=4
+ mkdir /root/data/
mkdir: cannot create directory '/root/data/': File exists
+ cp -r ./data/math /root/data/math
+ export ACCELERATE_LOG_LEVEL=info
+ ACCELERATE_LOG_LEVEL=info
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ export WANDB_CONSOLE=off
+ WANDB_CONSOLE=off
+ export WANDB_MODE=offline
+ WANDB_MODE=offline
+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ ray stop --force
2025-09-07 13:53:57,645	INFO scripts.py:1287 -- Did not find any active Ray processes.
+ export PET_NODE_RANK=0
+ PET_NODE_RANK=0
+ export WORLD_SIZE=1
+ WORLD_SIZE=1
+ PROJECT_NAME=self_rl_math
+ RUN_NAME=grpo_mv_naive_7b
++ date +%Y-%m-%d_%H-%M-%S
+ TIMESTAMP=2025-09-07_13-53-57
+ EXPERIMENT_NAME=self_rl_math-grpo_mv_naive_7b-2025-09-07_13-53-57
+ SAVE_CHECKPOINT_DIR=output_dir
+ EXP_DIR=output_dir/self_rl_math/self_rl_math-grpo_mv_naive_7b-2025-09-07_13-53-57
+ today=907
+ mkdir running_logs/907
mkdir: cannot create directory 'running_logs/907': File exists
+ mkdir -p output_dir
+ mkdir -p output_dir/self_rl_math
+ mkdir -p output_dir/self_rl_math/self_rl_math-grpo_mv_naive_7b-2025-09-07_13-53-57
+ mkdir -p output_dir/self_rl_math/self_rl_math-grpo_mv_naive_7b-2025-09-07_13-53-57
+ mkdir -p output_dir/logs/tensorboard
+ mkdir -p output_dir/logs/rl_logging_board
+ PYTHONUNBUFFERED=1
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=/root/data/math/train.parquet data.val_files=/root/data/math/test.parquet data.train_batch_size=128 data.max_prompt_length=512 data.max_response_length=3072 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=/inspire/hdd/project/robot-dna/sujiadi-p-sujiadi/baibizhe-tmp/ckpts/llm/Qwen2.5-7B actor_rollout_ref.model.use_fused_kernels=False actor_rollout_ref.actor.optim.lr=3e-6 actor_rollout_ref.actor.optim.warmup_style=cosine actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.1 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=128 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.005 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.3 actor_rollout_ref.rollout.n=12 actor_rollout_ref.rollout.free_cache_engine=False actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.n_gpus_per_node=8 reward_model.reward_manager=m_grpo trainer.nnodes=1 'trainer.logger=[console,wandb,tensorboard]' trainer.project_name=verl trainer.save_freq=10 trainer.val_before_train=False trainer.test_freq=10 trainer.project_name=self_rl_math trainer.experiment_name=self_rl_math-grpo_mv_naive_7b-2025-09-07_13-53-57 trainer.default_local_dir=output_dir/self_rl_math/self_rl_math-grpo_mv_naive_7b-2025-09-07_13-53-57 +trainer.tensorboard_dir=output_dir/logs/tensorboard +trainer.rl_logging_board_dir=output_dir/logs/rl_logging_board trainer.total_epochs=4
+ mkdir /root/data/
+ cp -r ./data/math /root/data/math
+ export ACCELERATE_LOG_LEVEL=info
+ ACCELERATE_LOG_LEVEL=info
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ export WANDB_CONSOLE=off
+ WANDB_CONSOLE=off
+ export WANDB_MODE=offline
+ WANDB_MODE=offline
+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ ray stop --force
2025-09-07 14:07:32,478	INFO scripts.py:1287 -- Did not find any active Ray processes.
+ export PET_NODE_RANK=0
+ PET_NODE_RANK=0
+ export WORLD_SIZE=1
+ WORLD_SIZE=1
+ PROJECT_NAME=self_rl_math
+ RUN_NAME=m_grpo_mv_naive_7b
++ date +%Y-%m-%d_%H-%M-%S
+ TIMESTAMP=2025-09-07_14-07-32
+ EXPERIMENT_NAME=self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-07-32
+ SAVE_CHECKPOINT_DIR=output_dir
+ EXP_DIR=output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-07-32
+ today=907
+ mkdir running_logs/907
mkdir: cannot create directory 'running_logs/907': File exists
+ mkdir -p output_dir
+ mkdir -p output_dir/self_rl_math
+ mkdir -p output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-07-32
+ mkdir -p output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-07-32
+ mkdir -p output_dir/logs/tensorboard
+ mkdir -p output_dir/logs/rl_logging_board
+ PYTHONUNBUFFERED=1
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=/root/data/math/train.parquet data.val_files=/root/data/math/test.parquet data.train_batch_size=128 data.max_prompt_length=512 data.max_response_length=3072 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=/inspire/hdd/project/robot-dna/sujiadi-p-sujiadi/baibizhe-tmp/ckpts/llm/Qwen2.5-7B actor_rollout_ref.model.use_fused_kernels=False actor_rollout_ref.actor.optim.lr=3e-6 actor_rollout_ref.actor.optim.warmup_style=cosine actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.1 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=128 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.005 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.3 actor_rollout_ref.rollout.n=12 actor_rollout_ref.rollout.free_cache_engine=False actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.n_gpus_per_node=8 reward_model.reward_manager=m_grpo trainer.nnodes=1 'trainer.logger=[console,wandb,tensorboard]' trainer.project_name=verl trainer.save_freq=10 trainer.val_before_train=False trainer.test_freq=10 trainer.project_name=self_rl_math trainer.experiment_name=self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-07-32 trainer.default_local_dir=output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-07-32 +trainer.tensorboard_dir=output_dir/logs/tensorboard +trainer.rl_logging_board_dir=output_dir/logs/rl_logging_board trainer.total_epochs=4
+ mkdir /root/data/
mkdir: cannot create directory '/root/data/': File exists
+ cp -r ./data/math /root/data/math
+ export ACCELERATE_LOG_LEVEL=info
+ ACCELERATE_LOG_LEVEL=info
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ export WANDB_CONSOLE=off
+ WANDB_CONSOLE=off
+ export WANDB_MODE=offline
+ WANDB_MODE=offline
+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ ray stop --force
2025-09-07 14:18:52,351	INFO scripts.py:1287 -- Did not find any active Ray processes.
+ export PET_NODE_RANK=0
+ PET_NODE_RANK=0
+ export WORLD_SIZE=1
+ WORLD_SIZE=1
+ PROJECT_NAME=self_rl_math
+ RUN_NAME=m_grpo_mv_naive_7b
++ date +%Y-%m-%d_%H-%M-%S
+ TIMESTAMP=2025-09-07_14-18-52
+ EXPERIMENT_NAME=self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-18-52
+ SAVE_CHECKPOINT_DIR=output_dir
+ EXP_DIR=output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-18-52
+ today=907
+ mkdir running_logs/907
mkdir: cannot create directory 'running_logs/907': File exists
+ mkdir -p output_dir
+ mkdir -p output_dir/self_rl_math
+ mkdir -p output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-18-52
+ mkdir -p output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-18-52
+ mkdir -p output_dir/logs/tensorboard
+ mkdir -p output_dir/logs/rl_logging_board
+ PYTHONUNBUFFERED=1
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=/root/data/math/train.parquet data.val_files=/root/data/math/test.parquet data.train_batch_size=128 data.max_prompt_length=512 data.max_response_length=3072 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=/inspire/hdd/project/robot-dna/sujiadi-p-sujiadi/baibizhe-tmp/ckpts/llm/Qwen2.5-7B actor_rollout_ref.model.use_fused_kernels=False actor_rollout_ref.actor.optim.lr=3e-6 actor_rollout_ref.actor.optim.warmup_style=cosine actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.1 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=128 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.005 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.3 actor_rollout_ref.rollout.n=12 actor_rollout_ref.rollout.free_cache_engine=False actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.n_gpus_per_node=8 reward_model.reward_manager=m_grpo trainer.nnodes=1 'trainer.logger=[console,wandb,tensorboard]' trainer.project_name=verl trainer.save_freq=10 trainer.val_before_train=False trainer.test_freq=10 trainer.project_name=self_rl_math trainer.experiment_name=self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-18-52 trainer.default_local_dir=output_dir/self_rl_math/self_rl_math-m_grpo_mv_naive_7b-2025-09-07_14-18-52 +trainer.tensorboard_dir=output_dir/logs/tensorboard +trainer.rl_logging_board_dir=output_dir/logs/rl_logging_board trainer.total_epochs=4
math_intuitor.sh: line 100: syntax error near unexpected token `&'
math_intuitor.sh: line 100: `&1'
+ mkdir /root/data/
mkdir: cannot create directory '/root/data/': File exists
+ cp -r ./data/math /root/data/math
+ export ACCELERATE_LOG_LEVEL=info
+ ACCELERATE_LOG_LEVEL=info
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ export WANDB_CONSOLE=off
+ WANDB_CONSOLE=off
+ export WANDB_MODE=offline
+ WANDB_MODE=offline
+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ ray stop --force
2025-09-09 03:20:46,844	INFO scripts.py:1287 -- Did not find any active Ray processes.
+ export PET_NODE_RANK=0
+ PET_NODE_RANK=0
+ export WORLD_SIZE=1
+ WORLD_SIZE=1
+ PROJECT_NAME=self_rl_math
+ RUN_NAME=grpo_mv_naive_3b
++ date +%Y-%m-%d_%H-%M-%S
+ TIMESTAMP=2025-09-09_03-20-46
+ EXPERIMENT_NAME=self_rl_math-grpo_mv_naive_3b-2025-09-09_03-20-46
+ SAVE_CHECKPOINT_DIR=output_dir
+ EXP_DIR=output_dir/self_rl_math/self_rl_math-grpo_mv_naive_3b-2025-09-09_03-20-46
+ today=909
+ mkdir running_logs/909
mkdir: cannot create directory 'running_logs/909': File exists
+ mkdir -p output_dir
+ mkdir -p output_dir/self_rl_math
+ mkdir -p output_dir/self_rl_math/self_rl_math-grpo_mv_naive_3b-2025-09-09_03-20-46
+ mkdir -p output_dir/self_rl_math/self_rl_math-grpo_mv_naive_3b-2025-09-09_03-20-46
+ mkdir -p output_dir/logs/tensorboard
+ mkdir -p output_dir/logs/rl_logging_board
+ PYTHONUNBUFFERED=1
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=/root/data/math/train.parquet data.val_files=/root/data/math/test.parquet data.train_batch_size=128 data.max_prompt_length=512 data.max_response_length=3072 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=/inspire/hdd/project/robot-dna/sujiadi-p-sujiadi/baibizhe-tmp/ckpts/llm/Qwen2.5-3B actor_rollout_ref.model.use_fused_kernels=False actor_rollout_ref.actor.optim.lr=1e-5 actor_rollout_ref.actor.optim.warmup_style=cosine actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.1 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=128 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.005 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.3 actor_rollout_ref.rollout.n=12 actor_rollout_ref.rollout.free_cache_engine=False actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.n_gpus_per_node=8 reward_model.reward_manager=m_grpo trainer.nnodes=1 'trainer.logger=[console,wandb,tensorboard]' trainer.project_name=verl trainer.save_freq=10 trainer.val_before_train=False trainer.test_freq=10 trainer.project_name=self_rl_math trainer.experiment_name=self_rl_math-grpo_mv_naive_3b-2025-09-09_03-20-46 trainer.default_local_dir=output_dir/self_rl_math/self_rl_math-grpo_mv_naive_3b-2025-09-09_03-20-46 +trainer.tensorboard_dir=output_dir/logs/tensorboard +trainer.rl_logging_board_dir=output_dir/logs/rl_logging_board trainer.total_epochs=4
+ python /inspire/hdd/project/robot-dna/sujiadi-p-sujiadi/baibizhe-tmp/do_gpu_work.py
Using 8 GPUs
